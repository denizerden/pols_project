{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLUQ0KEL610y"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4KkYghhG3IY"
      },
      "source": [
        "!pip install snowballstemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJiczoULfKOe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dtfGcr67a1a"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torch.utils import data\n",
        "\n",
        "\n",
        "\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import inflect\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "%matplotlib inline\n",
        "%config Inline.Backend.figure_formats='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette = 'muted', font_scale=1.2)\n",
        "\n",
        "rcParams['figure.figsize'] = 12,8\n",
        "\n",
        "RANDOM_SEED = 42 \n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_O_CsQA8y5N"
      },
      "source": [
        "#df = pd.read_csv('full_data.csv',error_bad_lines = False,encoding='utf-8',engine='python')\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/full_data.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsdgxdzM-xXb"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HllbUbO0-1yP"
      },
      "source": [
        "df.info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxdY6DyPyRue"
      },
      "source": [
        "df = df.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAVaizSayR6s"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqU07KpIySKC"
      },
      "source": [
        "df = df.drop(columns=['timestamp','unnamed_0'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax1TESwiTR8G"
      },
      "source": [
        "df_milligazete = df[df['brand']=='Milli Gazete']\n",
        "df_milligazete['length'] = df_milligazete['text'].str.len()\n",
        "df_milligazete.sort_values('length', ascending=False, inplace=True)\n",
        "df_milligazete = df_milligazete.groupby('title').text.agg('max').reset_index()\n",
        "df_milligazete[\"brand\"] = \"Milli Gazete\"\n",
        "df = df.drop(df[(df.brand=='Milli Gazete')].index)\n",
        "df_list = [df,df_milligazete]\n",
        "df = pd.concat(df_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWyFp99XTR6E"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6kY5OVFAD3O"
      },
      "source": [
        "stop_words =pd.read_excel('turkish_stop.xlsx')  \n",
        "stop_words = stop_words['word'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA37K6iSwMRm"
      },
      "source": [
        "df = df.fillna(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAkhcE-9nt5I"
      },
      "source": [
        "docs = np.array(df['text'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3EjIiGhaY-X"
      },
      "source": [
        "import itertools \n",
        "def grouper(n, iterable):\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        chunk = tuple(itertools.islice(it, n))\n",
        "        if not chunk:\n",
        "            return\n",
        "        yield chunk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfwhLQgzaY8z"
      },
      "source": [
        "import string\n",
        "def translate(doc):\n",
        "  new_list = []\n",
        "  tr2eng = str.maketrans(\"çğıöşüÇĞİÖÜ\", \"cgiosuCGIOU\")\n",
        "  for item in doc:\n",
        "      new_item = item.translate(tr2eng)\n",
        "      new_list.append(new_item)\n",
        "  return new_list\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BMZIQIspMCT"
      },
      "source": [
        "group = grouper(100,docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIBWF1C62O0Q"
      },
      "source": [
        "#def clean(doc):\n",
        "#    punc_free = []\n",
        "#    stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop_words])\n",
        "#    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "#    digit_free = re.sub(\"\\d+\", \" \", punc_free)\n",
        "    \n",
        "#    return digit_free\n",
        "import nltk\n",
        "import numpy as np\n",
        "WPT = nltk.WordPunctTokenizer()\n",
        "\n",
        "def norm_doc(single_doc):\n",
        "    \n",
        "    # Remove special characters and numbers\n",
        "    single_doc = re.sub(r\"([^a-zA-Z ]+?)\", \" \", single_doc)\n",
        "    pattern = r\"[{}]\".format(\",.;\") \n",
        "    single_doc = re.sub(pattern, \"\", single_doc) \n",
        "    #  Convert document to lowercase\n",
        "    single_doc = single_doc.lower()\n",
        "    single_doc = single_doc.strip()\n",
        "    #  Tokenize documents\n",
        "    tokens = WPT.tokenize(single_doc) \n",
        "    # EN: Filter out the stop-words \n",
        "\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  \n",
        "    #Reconstruct the document\n",
        "    single_doc = ' '.join(filtered_tokens)\n",
        "    return single_doc\n",
        "\n",
        "norm_docs = np.vectorize(norm_doc) #like magic :)\n",
        "normalized_documents = []\n",
        "while True:\n",
        "    try:\n",
        "        cleaned = next(group)\n",
        "        normalized_documents.append(norm_docs(translate(cleaned)))\n",
        "        \n",
        "    except StopIteration:\n",
        "        break\n",
        "    #print(group)\n",
        "#normalized_documents = norm_docs(docs_1[:1000])\n",
        "#print(normalized_documents)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVkqPmhzqeY_"
      },
      "source": [
        "data = []\n",
        "for doc in normalized_documents:\n",
        "  for item in doc:\n",
        "\n",
        "    strings = item.split(\",\")\n",
        "    data.append(strings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8o9HAI_wJio"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j35FPCny8kcY"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0DUJSgJ807K"
      },
      "source": [
        "new_row = []\n",
        "for item in data:\n",
        "  for i in item:\n",
        "    new_row.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdKaCJ9w0WDv"
      },
      "source": [
        "df[\"cleaned_text\"] = new_row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-2P8uaFncqB"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HGEAP7Q_smh"
      },
      "source": [
        "df.to_csv('cleaned_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaHtcDh1e1Sz"
      },
      "source": [
        "import locale\n",
        "from datetime import datetime\n",
        "\n",
        "def convert_datetime(value):\n",
        "    \n",
        "    #locale.setlocale(locale.LC_ALL, \"tr_TR\")\n",
        "    formats = ['%d %b %Y - %H:%M-', '%H:%M %d.%m.%Y', '%Y-%m-%d', '%d.%m.%Y - %H:%M','/%Y/%m/%d','%d-%m-%Y']\n",
        "    result_format = '%d-%m-%Y'\n",
        "    dt_obj = \"\"\n",
        "    for dt_format in formats:\n",
        "        \n",
        "        try:\n",
        "            dt_obj = datetime.strptime(value, dt_format).date()\n",
        "         \n",
        "            final_output =  datetime.strftime(dt_obj, \"%d-%m-%Y\")\n",
        "            \n",
        "         \n",
        "            #return dt_obj.strftime(result_format)\n",
        "        except ValueError:  # throws exception when format doesn't match\n",
        "            continue\n",
        "    return dt_obj  # let it be if it doesn't match\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehEumZ31e1S3"
      },
      "source": [
        "df = df.fillna(\"\")\n",
        "df.loc[:,\"date\"] = df.date.apply(lambda x: convert_datetime(x))\n",
        "df['datetime'] = pd.to_datetime(df['date'])\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.set_index('datetime')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vng1PR_yxE6A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM6gFYFpe1S6"
      },
      "source": [
        "df_milligazete = df[df['brand'] == 'Milli Gazete']\n",
        "df_sabah = df[df['brand'] == 'Sabah']\n",
        "df_sputnik = df[df['brand'] == 'Sputnik']\n",
        "df_hurriyet = df[df['brand'] == 'Hürriyet']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7StnGdHUvrv"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF8wrS7c5SdY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqspQef9MVHl"
      },
      "source": [
        "corpus=[]\n",
        "new= df['cleaned_text'].str.split()\n",
        "new=new.values.tolist()\n",
        "corpus=[word for i in new for word in i]\n",
        "\n",
        "from collections import defaultdict\n",
        "dic=defaultdict(int)\n",
        "for word in corpus:\n",
        "    if word in stop_words:\n",
        "        dic[word]+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR_sZQ0k4j-y"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJzS5vpcMVMZ"
      },
      "source": [
        "from collections import Counter\n",
        "counter=Counter(corpus)\n",
        "most=counter.most_common()\n",
        "\n",
        "x, y= [], []\n",
        "for word,count in most[:40]:\n",
        "    if (word not in stop_words):\n",
        "        x.append(word)\n",
        "        y.append(count)\n",
        "        \n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku3JNjQ5MVFv"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "def get_top_ngram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
        "    bag_of_words = vec.fit_transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) \n",
        "                  for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rv6148YMVD3"
      },
      "source": [
        "top_n_bigrams=get_top_ngram(df['cleaned_text'],2)[:10]\n",
        "x,y=map(list,zip(*top_n_bigrams))\n",
        "sns.barplot(x=y,y=x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHbI7moxMVBk"
      },
      "source": [
        "top_tri_grams=get_top_ngram(df['cleaned_text'],n=3)\n",
        "x,y=map(list,zip(*top_tri_grams))\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGbn9pK_BHCR"
      },
      "source": [
        "flat_list = []\n",
        "\n",
        "for sublist in normalized_documents:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nlza6BgC-FX"
      },
      "source": [
        "len(flat_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK3scBAEMU6O"
      },
      "source": [
        "from gensim.corpora import Dictionary\n",
        "import gensim\n",
        "BoW_Vector = CountVectorizer(min_df = 0., max_df = 1.)\n",
        "BoW_Matrix = BoW_Vector.fit_transform(flat_list)\n",
        "print(BoW_Matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3FoXuo_MU3m"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "number_of_topics = 4\n",
        "BoW_Matrix = BoW_Vector.fit_transform(flat_list)\n",
        "LDA = LatentDirichletAllocation(n_components = number_of_topics, \n",
        "                                max_iter = 10, \n",
        "                                learning_offset = 50.,\n",
        "                                random_state = 0,\n",
        "                                learning_method = 'online').fit(BoW_Matrix)\n",
        "features = BoW_Vector.get_feature_names()\n",
        "for t_id, topic in enumerate(LDA.components_):\n",
        "    print (\"Topic %d:\" % (t_id))\n",
        "    print (\" \".join([features[i]\n",
        "          for i in topic.argsort()[:-number_of_topics - 1:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJo8XCKiFx7E"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\",truncated=True)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\n",
        "sa= pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iznHtIPLFx0Q"
      },
      "source": [
        "def sentiment_score(doc):\n",
        "    \n",
        "    #try:\n",
        "    \n",
        "    if len(doc) > 512:\n",
        "        #print(doc)\n",
        "        \n",
        "        n = 512\n",
        "        chunks = [doc[i:i+n] for i in range(0, len(doc), n)]\n",
        "        scores = []\n",
        "        for chunk in chunks:\n",
        "            \n",
        "            \n",
        "            \n",
        "            p = sa(chunk)\n",
        "            #print(p)\n",
        "            if p[0]:\n",
        "                if p[0]['label'] == 'negative':\n",
        "                    score =  - p[0]['score']\n",
        "                    scores.append(score)\n",
        "                else:\n",
        "                    score = p[0]['score']\n",
        "                    scores.append(score)\n",
        "        #print(np.mean(scores))\n",
        "        return np.mean(scores)\n",
        "    else:\n",
        "        p = sa(doc)\n",
        "        score = 0\n",
        "        if p[0]['label'] == 'negative':\n",
        "            score =  - p[0]['score']\n",
        "        else:\n",
        "            score = p[0]['score']\n",
        "          \n",
        "        return score\n",
        "    #except:\n",
        "     #   return 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBfk6FTvFxyj"
      },
      "source": [
        "#df.loc[:,\"sentiment_score\"] = df.cleaned_text.apply(lambda x: sentiment_score(x))\n",
        "df_sabah = df_sabah.fillna(\"\")\n",
        "df_hurriyet = df_sabah.fillna(\"\")\n",
        "df_milligazete = df_sabah.fillna(\"\")\n",
        "df_sputnik = df_sabah.fillna(\"\")\n",
        "df_sabah.loc[:,\"sentiment_score\"] = df_sabah.cleaned_text.apply(lambda x: sentiment_score(x))\n",
        "print('finished')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKXtTIDyQXK"
      },
      "source": [
        "df_hurriyet.loc[:,\"sentiment_score\"] = df_hurriyet.cleaned_text.apply(lambda x: sentiment_score(x))\n",
        "print('finished')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZw-AcFKRk_h"
      },
      "source": [
        "df_milligazete2 = df_milligazete[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKISgIonyQcT"
      },
      "source": [
        "df_milligazete2.loc[:,\"sentiment_score\"] = df_milligazete2.cleaned_text.apply(lambda x: sentiment_score(x))\n",
        "print('finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqMT1-DmSbTH"
      },
      "source": [
        "df_milligazete2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNMWq3dUSyoX"
      },
      "source": [
        "df_milligazete.loc[:,\"sentiment_score\"] = df_milligazete.cleaned_text.apply(lambda x: sentiment_score(x))\n",
        "print('finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkiu41CbFBfW"
      },
      "source": [
        "df_milligazete.to_csv(\"milli_gazete_sent_scores.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2x47OweyQTn"
      },
      "source": [
        "df_sputnik.loc[:,\"sentiment_score\"] = df_sputnik.cleaned_text.apply(lambda x: sentiment_score(x))\n",
        "print('finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXnbE-qCt__A"
      },
      "source": [
        "df_milligazete.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeSzSPiRFxvy"
      },
      "source": [
        "df = pd.concat[df_sabah,df_milligazete,df_hurriyet,df_sputnik ]\n",
        "df.to_csv('sentiment_scores.csv')\n",
        "#df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anByzbkoN6a_"
      },
      "source": [
        "df['sentiment_score'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WRtpCnHY7V7"
      },
      "source": [
        "def sentiment(x):\n",
        "    if x<0:\n",
        "        return 'neg'\n",
        "    elif x==0:\n",
        "        return 'neu'\n",
        "    else:\n",
        "        return 'pos'\n",
        "    \n",
        "df['sentiment']=df['sentiment_score'].\\\n",
        "   map(lambda x: sentiment(x))\n",
        "\n",
        "plt.bar(df.sentiment.value_counts().index,\n",
        "        df.sentiment.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6anAPqyEOqPj"
      },
      "source": [
        "df[df['sentiment']=='pos']['cleaned_text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYItSHInOqaR"
      },
      "source": [
        "df[df['sentiment']=='neg']['cleaned_text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SDD0sjzOqiG"
      },
      "source": [
        "plt.plot(df_hurriyet.index, df_hurriyet['sentiment_score'])\n",
        "plt.title('Sentiment Scores Over Time')\n",
        "plt.ylabel('Sentiment Scores');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYKac10DOqf-"
      },
      "source": [
        "df[[\"sentiment_score\"]].resample(\"M\").median().plot(figsize=(15,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv5kLI9cOqel"
      },
      "source": [
        "df_hurriyet_q1 = df_hurriyet[(df_hurriyet['date'] > '2015-06-01 ') & (df_hurriyet['date'] <= '2015-10-01 ')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM7ozGl-SlOn"
      },
      "source": [
        "df_hurriyet_q1 = df_hurriyet_q1.set_index('date')\n",
        "df_hurriyet_q1.index = pd.to_datetime(df_hurriyet_q1.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY6m9PwzOqNO"
      },
      "source": [
        "df_hurriyet.plot(y=[\"sentiment_score\"], figsize=(15,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMszxt86OqLu"
      },
      "source": [
        "\n",
        "df_hurriyet_q1[[\"sentiment_score\"]].resample('MS', loffset=pd.Timedelta(14, 'd')).mean().plot(figsize=(15,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMXGFkQWOqIn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}